import requests
from bs4 import BeautifulSoup
import time
import csv
import random
import re
import os

# Láº¥y danh sÃ¡ch bÃ i viáº¿t tá»« trang BBC Vietnamese
def get_article_links(base_url="https://www.bbc.com/vietnamese/topics/ckdxnx1x5rnt?page=8", pages=1):
    article_links = []
    
    # ThÃªm User-Agent Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0'
    }
    
    for page in range(1, pages + 1):
        # ThÃªm tham sá»‘ trang náº¿u cáº§n
        page_url = base_url
        if page > 1:
            page_url = f"{base_url}/page/{page}"
        
        try:
            print(f"ğŸ“„ Äang táº£i trang {page}/{pages}: {page_url}")
            response = requests.get(page_url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f" Lá»—i khi truy cáº­p trang {page}: MÃ£ tráº¡ng thÃ¡i {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m táº¥t cáº£ cÃ¡c liÃªn káº¿t
            all_links = soup.find_all('a', href=True)
            print(f"TÃ¬m tháº¥y {len(all_links)} liÃªn káº¿t trÃªn trang {page}")
            
            # Pattern cho cÃ¡c bÃ i viáº¿t BBC
            article_pattern = re.compile(r'/vietnamese/(articles/[a-zA-Z0-9]+|[a-z\-]+/[a-zA-Z0-9\-]+)$')
            
            for a_tag in all_links:
                href = a_tag.get('href', '')
                
                # Kiá»ƒm tra chá»‰ láº¥y cÃ¡c bÃ i viáº¿t BBC Vietnamese
                if article_pattern.search(href):
                    # Äáº£m báº£o URL Ä‘áº§y Ä‘á»§
                    if href.startswith('/'):
                        full_url = f"https://www.bbc.com{href}"
                    else:
                        full_url = href
                    
                    # ThÃªm vÃ o danh sÃ¡ch náº¿u chÆ°a cÃ³
                    if full_url not in article_links:
                        article_links.append(full_url)
                        print(f"ğŸ” ÄÃ£ tÃ¬m tháº¥y liÃªn káº¿t bÃ i viáº¿t: {full_url}")
            
            # Thá»i gian chá» giá»¯a cÃ¡c trang
            if page < pages:
                delay = random.uniform(1, 3)
                print(f"â±ï¸ Chá» {delay:.2f} giÃ¢y trÆ°á»›c khi táº£i trang tiáº¿p theo...")
                time.sleep(delay)
                
        except Exception as e:
            print(f" Lá»—i khi tÃ¬m kiáº¿m bÃ i viáº¿t trang {page}: {e}")
    
    print(f"TÃ¬m tháº¥y tá»•ng cá»™ng {len(article_links)} bÃ i viáº¿t.")
    return article_links

# CÃ o ná»™i dung tá»« 1 bÃ i viáº¿t
def scrape_article_content(article_url):
    try:
        # ThÃªm User-Agent Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5'
        }
        
        # ThÃªm timeout vÃ  thá»­ láº¡i náº¿u cáº§n
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.get(article_url, headers=headers, timeout=10)
                if response.status_code == 200:
                    break
                print(f" Láº§n thá»­ {attempt+1}/{max_retries}: MÃ£ tráº¡ng thÃ¡i {response.status_code}")
                time.sleep(2)  # Chá» trÆ°á»›c khi thá»­ láº¡i
            except Exception as retry_error:
                print(f" Lá»—i khi thá»­ láº§n {attempt+1}/{max_retries}: {retry_error}")
                time.sleep(2)
                if attempt == max_retries - 1:
                    raise  # NÃ©m lá»—i náº¿u Ä‘Ã£ thá»­ háº¿t sá»‘ láº§n
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # TÃ¬m tiÃªu Ä‘á» - thá»­ nhiá»u lá»›p khÃ¡c nhau (BBC selectors)
        title = None
        for title_class in ['article-headline', 'gs-c-promo-heading__title', 'bbc-1ff6jq7']:
            title_tag = soup.find(['h1', 'h2'], class_=lambda c: c and title_class in c)
            if title_tag:
                title = title_tag.get_text(strip=True)
                print(f"ÄÃ£ tÃ¬m tháº¥y tiÃªu Ä‘á» sá»­ dá»¥ng class='{title_class}'")
                break
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y tiÃªu Ä‘á» báº±ng class, hÃ£y thá»­ tÃ¬m tháº» h1 Ä‘áº§u tiÃªn
        if not title:
            h1_tag = soup.find('h1')
            if h1_tag:
                title = h1_tag.get_text(strip=True)
                print(f"ÄÃ£ tÃ¬m tháº¥y tiÃªu Ä‘á» tá»« tháº» h1")
            else:
                title = "KhÃ´ng cÃ³ tiÃªu Ä‘á»"
                print(f" KhÃ´ng tÃ¬m tháº¥y tiÃªu Ä‘á» cho bÃ i viáº¿t: {article_url}")
        
        # TÃ¬m ná»™i dung - thá»­ nhiá»u lá»›p khÃ¡c nhau (BBC selectors)
        content = ""
        for content_class in ['bbc-19j92fr', 'bbc-1d18lk7', 'article-body-component', 'bbc-1cvxiy9']:
            content_container = soup.find(['div', 'article'], class_=lambda c: c and content_class in c)
            if content_container:
                content_tags = content_container.find_all(['p', 'div.text'])
                if content_tags:
                    content = '\n'.join(p.get_text(strip=True) for p in content_tags if p.get_text(strip=True))
                    print(f"ÄÃ£ tÃ¬m tháº¥y ná»™i dung sá»­ dá»¥ng class='{content_class}'")
                    break
        
        if not content:
            print(f" KhÃ´ng tÃ¬m tháº¥y ná»™i dung cho bÃ i viáº¿t: {article_url}")
            return None
        
        full_text = f"{title}\n{content}"
        print(f"ÄÃ£ cÃ o bÃ i viáº¿t thÃ nh cÃ´ng: {article_url}")
        return full_text
    
    except Exception as e:
        print(f" Lá»—i khi cÃ o bÃ i viáº¿t {article_url}: {e}")
        return None

# LÆ°u dá»¯ liá»‡u vÃ o file CSV
def save_to_csv(articles, website_url="https://www.bbc.com/vietnamese/", filename='articles.csv', append=True):
    if not articles:
        print(" KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u vÃ o file CSV.")
        return
    
    # Kiá»ƒm tra file cÃ³ tá»“n táº¡i khÃ´ng
    file_exists = os.path.isfile(filename)
    
    # Quyáº¿t Ä‘á»‹nh mode má»Ÿ file (append hay write)
    mode = 'a' if append and file_exists else 'w'
    
    with open(filename, mode=mode, encoding='utf-8', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=['content', 'website', 'label'])
        
        # Chá»‰ ghi header náº¿u file má»›i hoáº·c khÃ´ng append
        if mode == 'w':
            writer.writeheader()
            
        for article in articles:
            writer.writerow({
                'content': article,
                'website': website_url,
                'label': 1 
            })
    
    print(f"ÄÃ£ lÆ°u {len(articles)} bÃ i viáº¿t vÃ o file {filename}")
    if mode == 'a':
        print(f"Dá»¯ liá»‡u Ä‘Æ°á»£c thÃªm vÃ o file hiá»‡n cÃ³.")
    else:
        print(f"ÄÃ£ táº¡o file má»›i.")

# Main
if __name__ == "__main__":
    print("Báº¯t Ä‘áº§u cÃ o dá»¯ liá»‡u ")
    base_url = "https://www.bbc.com/vietnamese/topics/ckdxnx1x5rnt?page=8"
    
    links = get_article_links(base_url)
    
    if not links:
        print(" KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o. ThoÃ¡t chÆ°Æ¡ng trÃ¬nh.")
        exit()
    
    print(f"ğŸ”— TÃ¬m tháº¥y {len(links)} bÃ i viáº¿t. Báº¯t Ä‘áº§u cÃ o ná»™i dung...")
    
    articles = []
    max_articles = min(100, len(links))  # Giá»›i háº¡n sá»‘ bÃ i cÃ o
    
    for i, link in enumerate(links[:max_articles]):
        print(f"\n Äang cÃ o bÃ i {i+1}/{max_articles}: {link}")
        content = scrape_article_content(link)
        
        if content and len(content.split('\n')) > 1:  # Kiá»ƒm tra ná»™i dung cÃ³ há»£p lá»‡
            articles.append(content)
            print(f"ÄÃ£ cÃ o bÃ i thÃ nh cÃ´ng ({len(articles)}/{max_articles})")
        else:
            print(f" Bá» qua bÃ i khÃ´ng cÃ³ ná»™i dung há»£p lá»‡: {link}")
        
        # ThÃªm thá»i gian chá» ngáº«u nhiÃªn Ä‘á»ƒ giáº£m nguy cÆ¡ bá»‹ cháº·n
        delay = random.uniform(2, 5)
        print(f" Chá» {delay:.2f} giÃ¢y trÆ°á»›c khi cÃ o bÃ i tiáº¿p theo...")
        time.sleep(delay)
    
    # Há»i ngÆ°á»i dÃ¹ng cÃ³ muá»‘n append hay khÃ´ng
    append_option = input("Báº¡n cÃ³ muá»‘n thÃªm dá»¯ liá»‡u vÃ o file cÃ³ sáºµn khÃ´ng? (y/n): ").lower() == 'y'
    
    save_to_csv(articles, "https://www.bbc.com/vietnamese/", append=append_option)
    print("\nğŸ‰ HoÃ n thÃ nh quÃ¡ trÃ¬nh cÃ o dá»¯ liá»‡u!") 